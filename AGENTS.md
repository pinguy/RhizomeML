# Rhizome: Fine-Tuning Pipeline for Memory-Infused Dialogic AI

A modular pipeline for preparing, embedding, indexing, and fine-tuning dialogue data using LoRA and FAISS-backed memory on resource-constrained systems.

---

## üì¶ Prerequisites

**Platform:** Linux (tested in Distrobox)

**Minimum Specs:** CPU with 8 threads and 16‚ÄØGB RAM for training. You can reduce memory usage by adjusting the following values in `train_script.py`:

* `default_batch_size`
* `default_grad_accum`
* `per_device_train_batch_size`
* `gradient_accumulation_steps`

**Recommended:** No GPU required. With a few tweaks, it‚Äôll even run on a potato.

- OpenAI Codex should change these values to 2 when running in the environment and swap the model out for EleutherAI/gpt-neo-125m
---

## üîß Environment Setup

```bash
distrobox create --name rhizome-dev --image rhizome-devbox
distrobox enter rhizome-dev
```

Install dependencies:

```bash
# Install packages
pip3 install -r requirements.txt

# Download spacy English model
python3 -m spacy download en_core_web_sm

# Download NLTK data (for semantic processing)
python3 -c "import nltk; nltk.download('punkt_tab'); nltk.download('stopwords')"

# Download Vosk speech model
wget https://alphacephei.com/vosk/models/vosk-model-en-us-0.42-gigaspeech.zip
unzip vosk-model-en-us-0.42-gigaspeech.zip
```

---

## üìÇ Folder Structure

```
RhizomeML/
‚îú‚îÄ‚îÄ üìö Input Data
‚îÇ   ‚îú‚îÄ‚îÄ PDFs/                          # Place raw PDFs here
‚îÇ   ‚îú‚îÄ‚îÄ conversations.json             # ChatGPT export
‚îÇ   ‚îú‚îÄ‚îÄ conversations2.json            # Claude export (optional)
‚îÇ   ‚îú‚îÄ‚îÄ pdf_texts.json                 # PDFs JSON (optional)
‚îÇ   ‚îú‚îÄ‚îÄ pdf_to_json.py                 # PDF ‚Üí structured JSON
‚îÇ   ‚îú‚îÄ‚îÄ batch_embedder.py              # Embed & index memory
‚îÇ   ‚îú‚îÄ‚îÄ data_formatter.py              # Clean, dedupe, label, create datasets
‚îÇ   ‚îî‚îÄ‚îÄ train_script.py                # ‚ö° CPU-optimized QLoRA training
‚îÇ   ‚îú‚îÄ‚îÄ memory_texts.npy               # Embedded text vectors
‚îÇ   ‚îú‚îÄ‚îÄ memory_metadata.pkl            # Metadata for retrieval
‚îÇ   ‚îú‚îÄ‚îÄ semantic_memory.pkl            # Learned theme weights
‚îÇ   ‚îú‚îÄ‚îÄ data_finetune/                 # Training datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_train.jsonl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_validation.jsonl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_test.jsonl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_metadata.json      # Theme distribution stats
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenized_cache/           # ‚ö° Auto-cached tokenized data
‚îÇ   ‚îî‚îÄ‚îÄ DeepSeek-R1-Distill-Qwen-1.5B-finetuned/  # Model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ gradio_chat_tts.py             # STT ‚Üí LLM ‚Üí TTS interface
‚îÇ   ‚îú‚îÄ‚îÄ UCS_v3_4_1.py                  # UCS config
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
```

---

## üß± Pipeline Overview

### 1. Convert PDFs

```bash
python3 pdf_to_json.py
```

> Converts each PDF into chunked text with metadata.

---

### 2. Add Chat History

Rename your largest `conversations.json` export from ChatGPT (or other AI logs) and place it in the root folder.

---

### 3. Embed and Index Memory

```bash
python3 batch_embedder.py
```

> Creates semantic memory using FAISS, producing:

- `memory.index`
- `memory_texts.npy`
- `memory_metadata.pkl`

---

### 4. Generate Fine-Tuning Dataset

```bash
python3 data_formatter.py
```

> Cleans, deduplicates, and formats Q\&A pairs into `data_finetune/`.
> 
> If you run into issues, tweak `self.quality_score_threshold`.
> Higher values give fewer examples but better quality.

---

### 5. Train the Model (LoRA)

```bash
python3 data_formatter.py --force-cpu --enable-semantic-labeling --semantic-mode normal --semantic-method hybrid
```

> LoRA fine-tuning on Deepseek. Outputs go into `DeepSeek-R1-Distill-Qwen-1.5B-finetuned/`.

---

### 6. Chat with Your Fine-Tuned Model

```bash
python3 gradio_chat_tts.py
```

> Loads latest `checkpoint-*` from `DeepSeek-R1-Distill-Qwen-1.5B-finetuned/` and runs in an interactive loop.

---

## üß† Features

- Built for CPUs (LoRA + SentenceTransformer)
- Semantic memory recall using FAISS
- Fully reproducible from raw PDFs or chat history
- Modular and interpretable stages
- No reliance on proprietary APIs

---

## üç∑ Notes

- Works best with well-curated, conversational datasets
- Memory-backed recall enables enhanced introspective evaluation
- Logs and training diagnostics are saved automatically

---

## üìÑ License

This project is licensed under the WTFPL ‚Äì *Do What the Fuck You Want to Public License*.
See [wtfpl.net](http://www.wtfpl.net/) for more.

---

## Coding Conventions for OpenAI Codex

### General Conventions for Agents.md Implementation

- Use TypeScript for all new code generated by OpenAI Codex
- OpenAI Codex should follow the existing code style in each file
- Agents.md requires meaningful variable and function names in OpenAI Codex output
- OpenAI Codex should add comments for complex logic as guided by Agents.md
